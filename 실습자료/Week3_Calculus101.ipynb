{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d4d2aab",
      "metadata": {
        "id": "6d4d2aab"
      },
      "source": [
        "# **Deep Learning Programming with Python**\n",
        "### Week3: Calculus 101\n",
        "\n",
        "**Acknowledgement**\n",
        "\n",
        "Many of Today's contents are adopted from the below resources:\n",
        "\n",
        "1. https://d2l.ai/chapter_preliminaries/calculus.html\n",
        "2. https://colab.research.google.com/github/jigsawlabs-student/neural-nets-from-scratch/blob/master/2-training-proc/3-training-mathematically/4-partial-derivatives.ipynb#scrollTo=bUeRcd2OSdbX\n",
        "3. https://colab.research.google.com/github/ageron/handson-ml2/blob/master/math_differential_calculus.ipynb#scrollTo=s1lkPk08W6zp\n",
        "4. https://www.askpython.com/python/examples/derivatives-in-python-sympy\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217b0b41",
      "metadata": {
        "origin_pos": 6,
        "id": "217b0b41"
      },
      "source": [
        "## Derivatives and Differentiation\n",
        "\n",
        "Put simply, a *derivative* is the rate of change\n",
        "in a function with respect to changes in its arguments.\n",
        "Derivatives can tell us how rapidly a loss function\n",
        "would increase or decrease were we\n",
        "to *increase* or *decrease* each parameter\n",
        "by an infinitesimally small amount.\n",
        "Formally, for functions $f: \\mathbb{R} \\rightarrow \\mathbb{R}$,\n",
        "that map from scalars to scalars,\n",
        "[**the *derivative* of $f$ at a point $x$ is defined as**]\n",
        "\n",
        "(**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$**)\n",
        ":eqlabel:`eq_derivative`\n",
        "\n",
        "This term on the right hand side is called a *limit*\n",
        "and it tells us what happens\n",
        "to the value of an expression\n",
        "as a specified variable\n",
        "approaches a particular value.\n",
        "This limit tells us what\n",
        "the ratio between a perturbation $h$\n",
        "and the change in the function value\n",
        "$f(x + h) - f(x)$ converges to\n",
        "as we shrink its size to zero.\n",
        "\n",
        "When $f'(x)$ exists, $f$ is said\n",
        "to be *differentiable* at $x$;\n",
        "and when $f'(x)$ exists for all $x$\n",
        "on a set, e.g., the interval $[a,b]$,\n",
        "we say that $f$ is differentiable on this set.\n",
        "Not all functions are differentiable,\n",
        "including many that we wish to optimize,\n",
        "including accuracy and the area under the\n",
        "receiving operating characteristic (AUC).\n",
        "However, because computing the derivative of the loss\n",
        "is a crucial step in nearly all\n",
        "algorithms for training deep neural networks,\n",
        "we often optimize a differentiable *surrogate* instead.\n",
        "\n",
        "\n",
        "We can interpret the derivative\n",
        "$f'(x)$\n",
        "as the *instantaneous* rate of change\n",
        "of $f(x)$ with respect to $x$.\n",
        "Let's develop some intuition with an example.\n",
        "(**Define $u = f(x) = 3x^2-4x$.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bf6661e3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T05:17:16.977043Z",
          "iopub.status.busy": "2023-02-10T05:17:16.976417Z",
          "iopub.status.idle": "2023-02-10T05:17:16.992270Z",
          "shell.execute_reply": "2023-02-10T05:17:16.986322Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "bf6661e3"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da42c02",
      "metadata": {
        "origin_pos": 11,
        "id": "7da42c02"
      },
      "source": [
        "[**Setting $x=1$, $\\frac{f(x+h) - f(x)}{h}$**] (**approaches $2$\n",
        "as $h$ approaches $0$.**)\n",
        "\n",
        "\n",
        "While this experiment lacks\n",
        "the rigor of a mathematical proof,\n",
        "we will soon see that indeed $f'(1) = 2$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b685ba44",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T05:17:16.997725Z",
          "iopub.status.busy": "2023-02-10T05:17:16.997316Z",
          "iopub.status.idle": "2023-02-10T05:17:17.012902Z",
          "shell.execute_reply": "2023-02-10T05:17:17.006843Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "b685ba44",
        "outputId": "c4a08562-1e76-4e96-8dec-2cb825b6f626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h=0.10000, numerical limit=2.30000\n",
            "h=0.01000, numerical limit=2.03000\n",
            "h=0.00100, numerical limit=2.00300\n",
            "h=0.00010, numerical limit=2.00030\n",
            "h=0.00001, numerical limit=2.00003\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "for h in 10.0**np.arange(-1, -6, -1):\n",
        "    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c5c4c6",
      "metadata": {
        "origin_pos": 13,
        "id": "65c5c4c6"
      },
      "source": [
        "There are several equivalent notational conventions for derivatives.\n",
        "Given $y = f(x)$, the following expressions are equivalent:\n",
        "\n",
        "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
        "\n",
        "where the symbols $\\frac{d}{dx}$ and $D$ are *differentiation operators*.\n",
        "Below, we present the derivatives of some common functions:\n",
        "\n",
        "$$\\begin{aligned} \\frac{d}{dx} C & = 0 && \\text{for any constant $C$} \\\\ \\frac{d}{dx} x^n & = n x^{n-1} && \\text{for } n \\neq 0 \\\\ \\frac{d}{dx} e^x & = e^x \\\\ \\frac{d}{dx} \\ln x & = x^{-1} \\end{aligned}$$\n",
        "\n",
        "Functions composed from differentiable functions\n",
        "are often themselves differentiable.\n",
        "The following rules come in handy\n",
        "for working with compositions\n",
        "of any differentiable functions\n",
        "$f$ and $g$, and constant $C$.\n",
        "\n",
        "$$\\begin{aligned} \\frac{d}{dx} [C f(x)] & = C \\frac{d}{dx} f(x) && \\text{Constant multiple rule} \\\\ \\frac{d}{dx} [f(x) + g(x)] & = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x) && \\text{Sum rule} \\\\ \\frac{d}{dx} [f(x) g(x)] & = f(x) \\frac{d}{dx} g(x) + g(x) \\frac{d}{dx} f(x) && \\text{Product rule} \\\\ \\frac{d}{dx} \\frac{f(x)}{g(x)} & = \\frac{g(x) \\frac{d}{dx} f(x) - f(x) \\frac{d}{dx} g(x)}{g^2(x)} && \\text{Quotient rule} \\end{aligned}$$\n",
        "\n",
        "Using this, we can apply the rules\n",
        "to find the derivative of $3 x^2 - 4x$ via\n",
        "\n",
        "$$\\frac{d}{dx} [3 x^2 - 4x] = 3 \\frac{d}{dx} x^2 - 4 \\frac{d}{dx} x = 6x - 4.$$\n",
        "\n",
        "Plugging in $x = 1$ shows that, indeed,\n",
        "the derivative is $2$ at this location.\n",
        "Note that derivatives tell us\n",
        "the *slope* of a function\n",
        "at a particular location."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fb2a54",
      "metadata": {
        "origin_pos": 23,
        "id": "c0fb2a54"
      },
      "source": [
        "## Partial Derivatives and Gradients\n",
        "\n",
        "Thus far, we have been differentiating\n",
        "functions of just one variable.\n",
        "In deep learning, we also need to work\n",
        "with functions of *many* variables.\n",
        "We briefly introduce notions of the derivative\n",
        "that apply to such *multivariate* functions.\n",
        "\n",
        "\n",
        "Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with $n$ variables.\n",
        "The *partial derivative* of $y$\n",
        "with respect to its $i^\\mathrm{th}$ parameter $x_i$ is\n",
        "\n",
        "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
        "\n",
        "\n",
        "To calculate $\\frac{\\partial y}{\\partial x_i}$,\n",
        "we can treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ as constants\n",
        "and calculate the derivative of $y$ with respect to $x_i$.\n",
        "The following notation conventions for partial derivatives\n",
        "are all common and all mean the same thing:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = \\partial_{x_i} f = \\partial_i f = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n",
        "\n",
        "We can concatenate partial derivatives\n",
        "of a multivariate function\n",
        "with respect to all its variables\n",
        "to obtain a vector that is called\n",
        "the *gradient* of the function.\n",
        "Suppose that the input of function\n",
        "$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
        "is an $n$-dimensional vector\n",
        "$\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$\n",
        "and the output is a scalar.\n",
        "The gradient of the function $f$\n",
        "with respect to $\\mathbf{x}$\n",
        "is a vector of $n$ partial derivatives:\n",
        "\n",
        "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\left[\\partial_{x_1} f(\\mathbf{x}), \\partial_{x_2} f(\\mathbf{x}), \\ldots\n",
        "\\partial_{x_n} f(\\mathbf{x})\\right]^\\top.$$\n",
        "\n",
        "When there is no ambiguity,\n",
        "$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$\n",
        "is typically replaced\n",
        "by $\\nabla f(\\mathbf{x})$.\n",
        "The following rules come in handy\n",
        "for differentiating multivariate functions:\n",
        "\n",
        "* For all $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ we have $\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$ and $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$.\n",
        "* For square matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ we have that $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$ and in particular\n",
        "$\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$.\n",
        "\n",
        "Similarly, for any matrix $\\mathbf{X}$,\n",
        "we have $\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say that we have the following function:\n",
        "\n",
        "$f(x,y) = 3x^4y^2$\n",
        "\n",
        "So here, the output depends on two different values, $x$ and $y$.  And let's start by seeing how the output of $f(x,y)$ changes as we change the parameter $y$.\n",
        "\n",
        "> The notation for the partial derivative of $y$ is:\n",
        "> $$\\frac{\\delta f}{\\delta y}$$\n",
        "> That is, the change in our output of the function $f$ as we change our input $y$.\n",
        "> This is also worded as, the change in the function $f$, **with respect to** $y$."
      ],
      "metadata": {
        "id": "-qMzd-N1V4Yd"
      },
      "id": "-qMzd-N1V4Yd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with partial derivatives, if we have a function:\n",
        "\n",
        "* $f(x,y) = 3x^4y^2$, then\n",
        "* $\\frac{\\delta f}{\\delta y} = 2*3x^4y = 6x^4y$"
      ],
      "metadata": {
        "id": "iinlyM6vV4WD"
      },
      "id": "iinlyM6vV4WD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you see how we got there?  Let's see it again.\n",
        "\n",
        "* $f(x,y) = (3x^4)*y^2$\n",
        "* $\\frac{\\delta f}{\\delta y} = (3x^4)*2*y^1 = 6x^4y$"
      ],
      "metadata": {
        "id": "trfi36z8WWq5"
      },
      "id": "trfi36z8WWq5"
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, diff\n",
        "\n",
        "x, y = symbols('x y ', real=True)\n",
        "f = 3 * pow(x,4) * pow(y,2)\n",
        "diff(f, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "ZsB8CBGoUVUb",
        "outputId": "bdfef0bd-3e90-45e8-ce3e-29afbd97137e"
      },
      "id": "ZsB8CBGoUVUb",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6*x**4*y"
            ],
            "text/latex": "$\\displaystyle 6 x^{4} y$"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain Rule\n",
        "\n",
        "In deep learning, the gradients of concern\n",
        "are often difficult to calculate\n",
        "because we are working with\n",
        "deeply nested functions\n",
        "(of functions (of functions...)).\n",
        "Fortunately, the *chain rule* takes care of this.\n",
        "Returning to functions of a single variable,\n",
        "suppose that $y = f(g(x))$\n",
        "and that the underlying functions\n",
        "$y=f(u)$ and $u=g(x)$\n",
        "are both differentiable.\n",
        "The chain rule states that\n",
        "\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
        "\n",
        "\n",
        "\n",
        "Turning back to multivariate functions,\n",
        "suppose that $y = f(\\mathbf{u})$ has variables\n",
        "$u_1, u_2, \\ldots, u_m$,\n",
        "where each $u_i = g_i(\\mathbf{x})$\n",
        "has variables $x_1, x_2, \\ldots, x_n$,\n",
        "i.e.,  $\\mathbf{u} = g(\\mathbf{x})$.\n",
        "Then the chain rule states that\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\text{ and thus } \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,$$\n",
        "\n",
        "where $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ is a *matrix*\n",
        "that contains the derivative of vector $\\mathbf{u}$\n",
        "with respect to vector $\\mathbf{x}$.\n",
        "Thus, evaluating the gradient requires\n",
        "computing a vector-matrix product.\n",
        "This is one of the key reasons why linear algebra\n",
        "is such an integral building block\n",
        "in building deep learning systems.\n"
      ],
      "metadata": {
        "id": "axQML23ETmrj"
      },
      "id": "axQML23ETmrj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to chain many functions. For example, if $f(x)=g(h(i(x)))$, and we define $y=i(x)$ and $z=h(y)$, then $\\dfrac{\\mathrm{d}f}{\\mathrm{d}x} = \\dfrac{\\mathrm{d}f}{\\mathrm{d}z} \\dfrac{\\mathrm{d}z}{\\mathrm{d}y} \\dfrac{\\mathrm{d}y}{\\mathrm{d}x}$. Using Lagrange's notation, we get $f'(x)=g'(z)\\,h'(y)\\,i'(x)=g'(h(i(x)))\\,h'(i(x))\\,i'(x)$"
      ],
      "metadata": {
        "id": "x5q8BZWiW8On"
      },
      "id": "x5q8BZWiW8On"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say, we have a function h(x) = f( g(x) )\n",
        "\n",
        "Then according to chain rule: h′(x) = f ′(g(x)) g′(x)\n",
        "\n",
        "Example: f(x) = cos(x**2)"
      ],
      "metadata": {
        "id": "wRO8wYehXpc1"
      },
      "id": "wRO8wYehXpc1"
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sym\n",
        "#Chain Rule\n",
        "x = sym.Symbol('x')\n",
        "f = sym.cos(x**2)\n",
        "derivative_f = f.diff(x)\n",
        "derivative_f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "JKIw1RV4XjXk",
        "outputId": "8c2ab54b-691e-403e-e891-c34cc7a02111"
      },
      "id": "JKIw1RV4XjXk",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2*x*sin(x**2)"
            ],
            "text/latex": "$\\displaystyle - 2 x \\sin{\\left(x^{2} \\right)}$"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}